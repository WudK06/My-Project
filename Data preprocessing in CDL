# Import the essential libraries for data handling and numerical operations
import pandas as pd
import numpy as np
# Step 1: Loading the Data
# We'll load the CSV file into a pandas DataFrame.
# A DataFrame is like a smart spreadsheet that we can manipulate with code.
    df = pd.read_csv('my data.csv')
    print(f"Original shape of the dataset: {df.shape[0]} rows and {df.shape[1]} columns.")
print("-" * 50)
  # Step 2: Initial Exploration and Basic Cleaning 
# Let's peek at the first 5 rows to see what we're working with.
print("\nFirst 5 rows of the raw data:")
print(df.head())

# The .info() method gives a great summary of all columns, their data types, and how many non-missing values they have.
print("\nDataset Information (Columns, Data Types, Non-Nulls):")
df.info()

# From the .info() output and our file, we see a final column 'B' that seems
# to be an error from the CSV creation.
if 'B' in df.columns:
    df = df.drop(columns=['B'])
    print("\n'B' column not found, no action needed.")

 # Step 3: Handling Outliers and Inconsistent Data
# Let's get summary statistics to spot potential issues.
print("\nSummary statistics for numerical columns:")
print(df.describe().round(2))

# From the summary, we can see 'wt' might have some strange values.
# A maternal weight of 11.1kg is physiologically impossible for an adult.
# We'll define a "plausible" range (e.g., 35kg to 150kg) and fix values outside it.

wt_median = df['wt'].median()
plausible_wt_min = 35.0
plausible_wt_max = 150.0

# Find the rows with outlier weights
outliers_mask_wt = (df['wt'] < plausible_wt_min) | (df['wt'] > plausible_wt_max)
num_outliers_wt = outliers_mask_wt.sum()

if num_outliers_wt > 0:
    print(f"\nFound {num_outliers_wt} outlier(s) in the 'wt' column (e.g., values below {plausible_wt_min} kg).")
    # A good strategy is to replace these outliers with the median value of the column.
    # The median is robust to extreme values and is a safer choice than the mean.
    df.loc[outliers_mask_wt, 'wt'] = wt_median

# Correcting Blood Pressure ('BP') 
# Similarly, let's check the 'BP' column for impossible values.
# A diastolic blood pressure over 160 is extremely rare and could be a data entry error.
bp_median = df['BP'].median()
plausible_bp_max = 160.0

outliers_mask_bp = df['BP'] > plausible_bp_max
num_outliers_bp = outliers_mask_bp.sum()

if num_outliers_bp > 0:
    print(f"\nFound {num_outliers_bp} outlier(s) in the 'BP' column (e.g., values above {plausible_bp_max}).")
    df.loc[outliers_mask_bp, 'BP'] = bp_median
    print(f"✅ Replaced 'BP' outliers with the median value: {bp_median}.")
else:
    print("✅ No obvious outliers found in the 'BP' column.")

# Step 4: Separating Features (X) and Target (y) 

# Our target variable is 'LBW'.
y = df['LBW']

# Our features are all the other columns.
X = df.drop(columns=['LBW'])

print(f"✅ Target variable 'y' (LBW) has been created with {len(y)} records.")
print(f"✅ Feature set 'X' has been created with {X.shape[1]} columns.")
print("-" * 50)

  # Step 5: Splitting Data into Training and Testing Sets 

# We use scikit-learn's train_test_split function for this.
# test_size=0.2 means 20% of the data will be for testing.
# random_state=42 makes our split reproducible. Anyone who runs this code will get the exact same split.
# stratify=y is a crucial step for classification. It ensures that the proportion of LBW cases
# is the same in both the training and testing sets, which is important if one class is rarer than the other.

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)
print("✅ Data successfully split.")
print(f"Training set size: {len(X_train)} samples ({len(X_train)/len(df):.0%})")
print(f"Testing set size: {len(X_test)} samples ({len(X_test)/len(df):.0%})")
print("-" * 50)
  
 # Step 6: Scaling Numerical Features 
# We were scaled only the continuous numerical columns. We don't want to scale
# categorical identifiers (like ethnicity 'eth') or binary flags (like 'GestHypert').
numerical_features = ['ag', 'inc', 'age', 'wt', 'birthweght', 'GAd', 'BP']
print(f"Identified the following numerical features for scaling: {numerical_features}")

# We use StandardScaler, which transforms the data to have a mean of 0 and a standard deviation of 1.
scaler = StandardScaler()


# We fit the scaler ONLY on the training data. This learns the mean and standard deviation
# of our training set. We then use this fitted scaler to transform both the training AND testing sets.
# This prevents any information from the test set from "leaking" into our training process.

print("\nFitting the scaler on the training data...")
scaler.fit(X_train[numerical_features])

# Now, we transformed both sets.
X_train[numerical_features] = scaler.transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

print("✅ Training and testing sets have been successfully scaled.")

# Let's look at the first 5 rows of our fully preprocessed training data.
print("\nFirst 5 rows of the final, preprocessed training data (X_train):")
print(X_train.head())

print("Your data is now fully preprocessed and ready for model training.")
print("You have the following sets available:")
print(" - X_train: Features for training the model.")
print(" - y_train: Target labels for training the model.")
print(" - X_test: Features for evaluating the model's performance on unseen data.")
print(" - y_test: Target labels for evaluating the model.")
