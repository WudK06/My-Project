import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 1. Data Loading and Preprocessing 
    df = pd.read_csv('my data.csv')
# 2. Normalized Importance of Variables 
# Define the specific features for this analysis
selected_features = [
    'MecoAsp', 'PerinatalDeath', 'GestHypert', 'VacResusc',
    'PrevSurg', 'GestAge_cat', 'MotherAlloc'
]
# Ensure all columns exist before proceeding
missing_cols = [col for col in selected_features if col not in df.columns]
if missing_cols:
    print(f"Error: The following columns for importance analysis were not found: {missing_cols}")
else:
    X_imp = df[selected_features]
    y_imp = df['LBW']

    # Train a RandomForest model to get feature importances
    model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
    model_rf.fit(X_imp, y_imp)

    # Create a DataFrame of importances
    importance_df = pd.DataFrame({
        'Variable': X_imp.columns,
        'Importance': model_rf.feature_importances_
    }).sort_values(by='Importance', ascending=False)

    # Normalize the importance scores
    importance_df['Normalized Importance'] = importance_df['Importance'] / importance_df['Importance'].max()

    print("Normalized Feature Importances:")
    print(importance_df[['Variable', 'Normalized Importance']].round(3))

    # Plotting the feature importances
    plt.style.use('seaborn-v0_8-talk')
    fig, ax = plt.subplots(figsize=(12, 8))
    sns.barplot(x='Normalized Importance', y='Variable', data=importance_df, palette='coolwarm', ax=ax)
    ax.set_title(' Normalized Importance of Variables', fontsize=18, pad=20)
    ax.set_xlabel('Importance', fontsize=14)
    ax.set_ylabel('')
    for index, value in enumerate(importance_df['Normalized Importance']):
        ax.text(value + 0.005, index, f'{value:.3f}', va='center')
    plt.tight_layout()
    plt.show()
# 3. Power of Receiver Operating Characteristic (ROC) 
# Define features (all columns except LBW) and target
y = df['LBW']
X = df.drop('LBW', axis=1)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)

# Identify numerical features for scaling
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
scaler = StandardScaler()

# Fit scaler on training data and transform both sets
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

# Train a logistic regression model
model_logreg = LogisticRegression(solver='liblinear', random_state=42)
model_logreg.fit(X_train, y_train)

# Predict probabilities for the positive class (LBW=1)
y_pred_proba = model_logreg.predict_proba(X_test)[:, 1]
# Calculate ROC curve and AUC
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)
print(f"Area Under ROC Curve (AUC): {roc_auc:.4f}")

# Plot the ROC Curve
plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'Model (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1 - Specificity (False Positive Rate)', fontsize=14)
plt.ylabel('Sensitivity (True Positive Rate)', fontsize=14)
plt.title('Power of Receiver Operating Characteristic Curves', fontsize=16)
plt.legend(loc="lower right")
plt.show()

# 4. Neural Network Parameter Estimates 
    # Prepare the specific data for the neural network
    X_nn = df[selected_features]
    y_nn = df['LBW']
    
    # Neural networks are sensitive to feature scaling
    scaler_nn = StandardScaler()
    X_nn_scaled = scaler_nn.fit_transform(X_nn)

    # Build the 3-layer MLP model 
    model_mlp = Sequential([
        Dense(5, activation='tanh', input_shape=(X_nn.shape[1],), name='hidden_layer_1'),
        Dense(1, activation='sigmoid', name='output_layer')
    ])
    
    # Compile the model
    model_mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    # Train the model
    model_mlp.fit(X_nn_scaled, y_nn, epochs=50, batch_size=32, verbose=0)
    print("3-Layer MLP model trained successfully.")

    # Extract weights and biases
    hidden_layer_weights, hidden_layer_biases = model_mlp.get_layer('hidden_layer_1').get_weights()
    output_layer_weights, output_layer_biases = model_mlp.get_layer('output_layer').get_weights()

    # Create the parameter estimates table 
    # Note: LBW=0 is Normal, LBW=1 is Low Birth Weight. The model predicts P(LBW=1).
    # A positive weight to the output layer favors LBW=1, a negative weight favors LBW=0.
    # The document seems to have reversed this (LBW=0 is Low, NBW=1 is Normal).
    
    # Input Layer to Hidden Layer 1
    input_to_hidden_df = pd.DataFrame(
        data=hidden_layer_weights,
        index=selected_features,
        columns=[f'H(1:{i+1})' for i in range(5)]
    )
    bias_df = pd.DataFrame([hidden_layer_biases], index=['Bias'], columns=[f'H(1:{i+1})' for i in range(5)])
    input_to_hidden_df = pd.concat([bias_df, input_to_hidden_df])
    
    # Hidden Layer 1 to Output Layer
    hidden_to_output_df = pd.DataFrame(
        data=output_layer_weights.flatten(),
        index=[f'H(1:{i+1})' for i in range(5)],
        columns=['Weight to Output']
    )
    output_bias_df = pd.DataFrame({'Weight to Output': output_layer_biases}, index=['Bias'])
    hidden_to_output_df = pd.concat([output_bias_df, hidden_to_output_df])

    print("\nInput Layer to Hidden Layer 1 Weights:")
    print(input_to_hidden_df.round(3))
    print("\nHidden Layer 1 to Output Layer Weights (Predicting P(LBW=1)):")
    # Positive weights increase probability of LBW=1 (Low Birth Weight)
    # Negative weights increase probability of LBW=0 (Normal Birth Weight)
    print(hidden_to_output_df.round(3))

Based on the above Python code the following are a bit explanations	
# Normalized Importance of Variables
The code trains a Random Forest model, which is an ensemble of decision trees, to determine which of your selected features are most predictive of LBW.
Output Table: The script will print a table showing the normalized importance of each variable.
The variable with the highest score is the most influential in the model's predictions.
Output Plot: A bar chart visually ranks the variables. We can directly compare this to your "Normalized Importance of Variables". 
The results should be very similar, confirming that Gestational Age, Meconium Aspiration, and Perinatal Death are the most critical factors among this set.
# Power of Receiver Operating Characteristic (ROC) 
This section evaluates how well a classification model can distinguish between Low Birth Weight (LBW=1) and Normal Birth Weight (LBW=0) cases.
AUC Score: The code will print the AUC score, which is a single number summarizing the model's performance. 
An AUC of 0.88 (as seen in the document) is considered excellent. The result should be close to this value.
Output Plot: The generated plot will show the True Positive Rate vs. the False Positive Rate.
# Neural Network Parameter Estimates (Figure & Table )
Model Architecture: The code builds a neural network with 7 input layer (7 variables), 5 hidden layer (5 neurons), 
and one output layer (prediction).
Output Tables:
Input to Hidden Layer: This table shows the "synaptic weights" connecting each of the 7 input variables to the 5 neurons in the hidden layer.
A strong positive or negative weight indicates a strong influence. For example, We can look at the column for H(1:2) 
and see the large positive weight (e.g., ~0.728 in your document) coming from MecoAsp, indicating this feature strongly activates that specific hidden neuron.
Hidden to Output Layer: This table shows the weights connecting the 5 hidden neurons to the final output.
In our model, a positive weight from a hidden neuron increases the final probability of LBW=1 (Low Birth Weight). 
A negative weight pushes the prediction towards LBW=0 (Normal Birth Weight). 

